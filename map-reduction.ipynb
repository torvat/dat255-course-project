{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduction\n",
    "A good way to work around token limitations. If you are summarizing text that is too long for your chosen model's context window, Map Reduction can be used.\n",
    "\n",
    "Map Reduction works like this:\n",
    "- The text is broken up into manageable pieces (chunks)\n",
    "- A summary is generated for each chunk\n",
    "- A final summary is generated from all the chunk summaries\n",
    "\n",
    "This method is well suited for generating summaries of some types of text, but has some limitations:\n",
    "- The model may over or underemphazise certain aspects of the text\n",
    "- Gets really expensive really fast, as you need many calls to the LLM and lots of input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import read_files\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "documents = read_files(Path('./content/books'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_map_template = \"\"\"Write a short summary of the following text:\n",
    "\n",
    "{context}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "summary_reduce_template = \"\"\"The following text is a set of summaries:\n",
    "\n",
    "{doc_summaries}\n",
    "\n",
    "Create a cohesive summary from the above text.\n",
    "SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "def split_document_by_tokens(document: list[Document], chunk_size: int, overlap: int):\n",
    "    splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    return splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def summarize_document(document: list[Document]):\n",
    "    # Chain to generate a summary from each chunk\n",
    "    map_prompt = PromptTemplate.from_template(summary_map_template)\n",
    "    map_chain = LLMChain(prompt=map_prompt, llm=llm)\n",
    "\n",
    "    # Chain to generate one cohesive summary from the summaries\n",
    "    reduce_prompt = PromptTemplate.from_template(summary_reduce_template)\n",
    "    reduce_chain = LLMChain(prompt=reduce_prompt, llm=llm)\n",
    "    stuff_chain = StuffDocumentsChain(llm_chain=reduce_chain, document_variable_name=\"doc_summaries\")\n",
    "    reduce_docs_chain = ReduceDocumentsChain(combine_documents_chain=stuff_chain)\n",
    "\n",
    "    # The complete map reduction chain\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        llm_chain=map_chain,\n",
    "        document_variable_name=\"content\",\n",
    "        reduce_documents_chain=reduce_docs_chain\n",
    "    )\n",
    "\n",
    "    splitdocs = split_document_by_tokens(document, 15000, 200)\n",
    "    summary = map_reduce_chain.run(splitdocs)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
